{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import datetime as dt\n",
    "import csv\n",
    "\n",
    "from scipy import sparse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression, LogisticRegression, RidgeClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Loading dataset + defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_df = pd.read_csv('datasets\\\\science_dataset_updated2.csv') #Loads dataset\n",
    "sports_df = pd.read_csv('datasets\\\\sports_dataset_updated2.csv') #Loads dataset\n",
    "gaming_df = pd.read_csv('datasets\\\\gaming_dataset_updated2.csv') #Loads dataset\n",
    "wsb_df = pd.read_csv('datasets\\\\wsb_dataset_updated2.csv') #Loads dataset\n",
    "\n",
    "indexNames = gaming_df[gaming_df['score'] < 2].index\n",
    "indexNamessc = science_df[science_df['score'] < 2].index\n",
    "indexNamessp = sports_df[sports_df['score'] < 2].index\n",
    "\n",
    "gaming_df_clean = gaming_df.drop(indexNames, axis=0)\n",
    "science_df_clean = science_df.drop(indexNamessc, axis=0)\n",
    "sports_df_clean = sports_df.drop(indexNamessp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sports 76.95814350797266\n",
      "gaming 52.2487862551254\n",
      "science 135.01687956204378\n",
      "wsb 60.337679269882656\n"
     ]
    }
   ],
   "source": [
    "print('sports', sports_df['title length'].mean())\n",
    "print('gaming', gaming_df['title length'].mean())\n",
    "print('science', science_df['title length'].mean())\n",
    "print('wsb', wsb_df['title length'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_flair = pd.get_dummies(df['flair'])\n",
    "df = science_df\n",
    "df['24h_posttime'] = df['24h_posttime'].astype(str)\n",
    "dummy_24h = pd.get_dummies(df['24h_posttime'])\n",
    "\n",
    "gaming_df_clean = df.drop(['post_ID', 'url', 'author', 'timestamp', 'permalink', 'body', 'Flair', '24h_posttime', 'title length', 'title_cleaned', 'score_class', 'has_body_text', 'comms_num'], axis=1)\n",
    "science_df_clean = df.drop(['post_ID', 'url', 'author', 'timestamp', 'permalink', 'body', 'Flair', '24h_posttime', 'title length', 'title_cleaned', 'score_class', 'has_body_text', 'comms_num'], axis=1)\n",
    "sports_df_clean = df.drop(['post_ID', 'url', 'author', 'timestamp', 'permalink', 'body', 'Flair', '24h_posttime', 'title length', 'title_cleaned', 'score_class', 'has_body_text', 'comms_num'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "plot = sns.boxplot(x = gaming_df_clean.score[gaming_df_clean.score < 50], ax=ax)\n",
    "plot.set_title('Cleaned r/gaming score distribution', fontsize = 18)\n",
    "plot.tick_params(labelsize=14)\n",
    "plot.set_xlabel('Score', fontsize = 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "plot = sns.boxplot(x = gaming_df.score[gaming_df.score < 50], ax=ax)\n",
    "plot.set_title('original r/gaming score distribution', fontsize = 18)\n",
    "plot.tick_params(labelsize=14)\n",
    "plot.set_xlabel('Score', fontsize = 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_df = science_df.drop(['24h_posttime', 'comms_num'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,10)) \n",
    "heatmap  = sns.heatmap(science_df.corr(), ax=ax, annot=True)\n",
    "heatmap.set_title('Heatmap for: Title length & Body text', fontdict={'fontsize':16}, pad=12)\n",
    "heatmap.tick_params(labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_vect = vectorize_dataframe(science_df)\n",
    "gaming_vect = vectorize_dataframe(gaming_df)\n",
    "sports_vect = vectorize_dataframe(sports_df)\n",
    "wsb_vect = vectorize_dataframe(wsb_df)\n",
    "\n",
    "print(science_vect.shape)\n",
    "print(gaming_vect.shape)\n",
    "print(sports_vect.shape)\n",
    "print(wsb_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) #Defines stopwords\n",
    "ps = PorterStemmer() #Defines stemmer\n",
    "\n",
    "def preprocess_text_col(dataframe, column_name): #Function for preprocessing text data for model-use by adding 'title-cleaned' column to given dataframe\n",
    "    def remove_punctuation(text): #Removes punctuation from string e.g. 'This is a string. This is another string' → 'this is a string This is another string' \n",
    "        no_punct=[words.lower() for words in text if words not in string.punctuation and words.isdigit() == False]\n",
    "        words_wo_punct=''.join(no_punct)\n",
    "        return words_wo_punct\n",
    "    def tokenize(text): #Tokenizes string e.g. 'This is a string' → ['this', 'is', 'a', 'string']\n",
    "        split=re.split(\"\\W+\", text)\n",
    "        return split\n",
    "    def remove_stopwords(text): #Removes stopwords list of strings e.g. ['this', 'is', 'a', 'string'] → ['string']\n",
    "        text=[word for word in text if word not in stop_words and word != '']\n",
    "        return text\n",
    "    def stem_nested_list(lst): #Stems words in a nested list and returns a nested list with stemmed words\n",
    "        master_list = []\n",
    "        for x in lst:\n",
    "            stemmed_list = [ps.stem(word) for word in x]\n",
    "            master_list.append(stemmed_list)\n",
    "        return master_list\n",
    "    \n",
    "    title_wo_punct = [remove_punctuation(x) for x in dataframe[column_name]]\n",
    "    title_wo_punct_split = [tokenize(word) for word in title_wo_punct]\n",
    "    title_wo_punct_split_stopwords = [remove_stopwords(word) for word in title_wo_punct_split]\n",
    "    dataframe['title_cleaned'] = title_wo_punct_split_stopwords\n",
    "#     dataframe['title_cleaned'] = stem_nested_list(title_wo_punct_split_stopwords)    \n",
    "\n",
    "def create_features(dataframe):\n",
    "    \n",
    "    dataframe['timestamp'] = pd.to_datetime(dataframe['timestamp']) #Changing 'timestamp' column to dtype = datetime\n",
    "    dataframe['24h_posttime'] = dataframe['timestamp'].dt.hour #Adding hour posttime to dataset\n",
    "    \n",
    "    dataframe['title length'] = [len(x) for x in dataframe.title]\n",
    "    \n",
    "    dataframe['body'] = dataframe['body'].astype(str)\n",
    "    dataframe.loc[(dataframe['body'] == 'nan') | (dataframe['body'] == '[deleted]'), 'has_body_text'] = int(0) \n",
    "    dataframe.loc[(dataframe['body'] != 'nan') & (dataframe['body'] != '[deleted]'), 'has_body_text'] = int(1)\n",
    "#     dataframe['has_body_text'] = dataframe['has_body_text'].astype(int)\n",
    "\n",
    "def create_score_class(dataframe):\n",
    "    dataframe.reset_index(inplace=True)\n",
    "    dataframe['over_50'] = 0 #Creating the score_class column in the dataframe and filling it with empty 0s\n",
    "    for x in range(len(dataframe)): #Generates classes for upvote class\n",
    "        if dataframe['score'][x] >= 50:\n",
    "            dataframe['over_50'][x] = 1\n",
    "        else:\n",
    "            dataframe['over_50'][x] = 0\n",
    "    \n",
    "def test_model(model): #Function for testing model(s)\n",
    "    if type(model) == list:\n",
    "        for x in range(len(model)):\n",
    "            print(\"Training score for {}: {:.3f}\".format(str(model[x]), model[x].score(X_train, y_train)))\n",
    "            print(\"Test score for {}: {:.2f}\\n\".format(str(model[x]), model[x].score(X_test, y_test)))\n",
    "    else:\n",
    "        print(\"Training score for {}: {:.3f}\".format(str(model), model.score(X_train, y_train)))\n",
    "        print(\"Test score for {}: {:.2f}\".format(str(model), model.score(X_test, y_test)))\n",
    "        \n",
    "def col_to_matrix(dataframe, column): #Function for converting a column from a pd.dataframe into a scipy.sparse.csr_matrix\n",
    "    matrix = dataframe[column].values[np.newaxis] #Creating 2D np array from column by adding an axis to original 1D array (df[col].values)\n",
    "    matrix = matrix.T #Transposing (rotating) array e.g. (1, 823) to (823, 1)\n",
    "    matrix = sparse.csr_matrix(matrix) #Creating matrix from array\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Preprocessing text-data for model-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_time = dt.datetime.now()\n",
    "\n",
    "vectorizerbow_out = CountVectorizer(lowercase = False, analyzer=lambda x: x)\n",
    "\n",
    "\n",
    "def vectorize_dataframe(dataframe):\n",
    "    post_time = col_to_matrix(dataframe, '24h_posttime')\n",
    "    title_len = col_to_matrix(dataframe, 'title length')\n",
    "    has_body_text = col_to_matrix(dataframe, 'has_body_text')\n",
    "    preprocess_text_col(dataframe, 'title')\n",
    "\n",
    "    vectorizerbow = CountVectorizer(lowercase = False, analyzer=lambda x: x)\n",
    "    tfidfvectorizer = TfidfVectorizer(lowercase = False, analyzer=lambda x: x)\n",
    "\n",
    "    vectorizer = vectorizerbow\n",
    "    titles_vectorized_bow = vectorizer.fit_transform(dataframe['title_cleaned'])\n",
    "\n",
    "    titles_vectorized_bow = sparse.hstack((post_time, titles_vectorized_bow)) #Adding posttime column to matrix\n",
    "    titles_vectorized_bow = sparse.hstack((title_len, titles_vectorized_bow)) #Adding title_length column to matrix\n",
    "    titles_vectorized_bow = sparse.hstack((has_body_text, titles_vectorized_bow)) #Adding has_body_text column to matrix\n",
    "    return titles_vectorized_bow, vectorizerbow\n",
    "\n",
    "print('Time spent (hh:mm:ss):', dt.datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Creating, training and testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=[0, 1, 2, 3, 4],  = [wsb_0.score.mean(), wsb_2.score.mean(), wsb_3.score.mean(), wsb_4.score.mean(), wsb_5.score.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_vocab_sorted = dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])) # Sorting dictionary of word:vector_index by vector_index\n",
    "titles_vectorized_bow_df = pd.DataFrame(titles_vectorized_bow.toarray()) # Creating DataFrame from vectorized titles\n",
    "titles_vectorized_bow_df = titles_vectorized_bow_df.drop([0, 1, 2], axis=1) # Removing 'title_length', 'has_body_text' & '24h_posstime' columns\n",
    "\n",
    "vocab_dict = {}\n",
    "\n",
    "sum_vect = titles_vectorized_bow_df.sum(axis=0) #dataframe with sums of all vects across all rows\n",
    "\n",
    "for x in range(len(vect_vocab_sorted)): #Creating a dictionary consisting of word:sum pairs\n",
    "    vocab_dict[list(vect_vocab_sorted)[x]] = list(sum_vect)[x]\n",
    "\n",
    "sorted_vocab = dict(sorted(vocab_dict.items(), key=lambda item: item[1], reverse=True)) #Sorting the word:sum pairs by sum\n",
    "sorted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text_col(gaming_df, 'title')\n",
    "\n",
    "target = gaming_df.score_class\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorize_dataframe(gaming_df), target, test_size = 0.2, random_state = 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with ngram_range: (1, 1) & alpha = 0.01\n",
      "Train: 0.9933611999016474\n",
      "Test: 0.6184857423795477\n",
      "Results with ngram_range: (1, 1) & alpha = 0.1\n",
      "Train: 0.9933611999016474\n",
      "Test: 0.6125860373647984\n",
      "Results with ngram_range: (1, 1) & alpha = 1\n",
      "Train: 0.9921317924760266\n",
      "Test: 0.6234021632251721\n",
      "Results with ngram_range: (1, 1) & alpha = 5\n",
      "Train: 0.9650848291123678\n",
      "Test: 0.6450344149459194\n",
      "Results with ngram_range: (1, 1) & alpha = 10\n",
      "Train: 0.9301696582247356\n",
      "Test: 0.6529006882989183\n",
      "Results with ngram_range: (1, 1) & alpha = 20\n",
      "Train: 0.8861568723875092\n",
      "Test: 0.6548672566371682\n",
      "ridge done. Time spent so far...  0:00:00.780934 \n",
      "\n",
      "Results with ngram_range: (1, 1) & alpha = 1\n",
      "Train: 0.9933611999016474\n",
      "Test: 0.6411012782694199\n",
      "Results with ngram_range: (1, 1) & alpha = 2\n",
      "Train: 0.7032210474551266\n",
      "Test: 0.6588003933136677\n",
      "Results with ngram_range: (1, 1) & alpha = 3\n",
      "Train: 0.7118268994344726\n",
      "Test: 0.6578171091445427\n",
      "Results with ngram_range: (1, 1) & alpha = 4\n",
      "Train: 0.6727317432997295\n",
      "Test: 0.6627335299901671\n",
      "Results with ngram_range: (1, 1) & alpha = 5\n",
      "Train: 0.6845340545856897\n",
      "Test: 0.6617502458210422\n",
      "Results with ngram_range: (1, 1) & alpha = 6\n",
      "Train: 0.6680599950823704\n",
      "Test: 0.6597836774827925\n",
      "Results with ngram_range: (1, 1) & alpha = 7\n",
      "Train: 0.6702729284484878\n",
      "Test: 0.6588003933136677\n",
      "Tree count = 20      running...\n",
      "Tree count = 50      running...\n",
      "Tree count = 70      running...\n",
      "Tree count = 100      running...\n",
      "Results with ngram_range: (1, 1) & alpha = 50\n",
      "Train: 0.8657487091222031\n",
      "Test: 0.6263520157325467\n",
      "Results with ngram_range: (1, 1) & alpha = 100\n",
      "Train: 0.9173838209982789\n",
      "Test: 0.6243854473942969\n",
      "Results with ngram_range: (1, 1) & alpha = 200\n",
      "Train: 0.9702483402999754\n",
      "Test: 0.5968534906588004\n",
      "Results with ngram_range: (1, 1) & alpha = 300\n",
      "Train: 0.9933611999016474\n",
      "Test: 0.5899705014749262\n",
      "\n",
      "______________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Results with ngram_range: (1, 2) & alpha = 0.01\n",
      "Train: 0.9936070813867716\n",
      "Test: 0.647984267453294\n",
      "Results with ngram_range: (1, 2) & alpha = 0.1\n",
      "Train: 0.9936070813867716\n",
      "Test: 0.6489675516224189\n",
      "Results with ngram_range: (1, 2) & alpha = 1\n",
      "Train: 0.9936070813867716\n",
      "Test: 0.6489675516224189\n",
      "Results with ngram_range: (1, 2) & alpha = 5\n",
      "Train: 0.9928694369313991\n",
      "Test: 0.6470009832841691\n",
      "Results with ngram_range: (1, 2) & alpha = 10\n",
      "Train: 0.9904106220801574\n",
      "Test: 0.6548672566371682\n",
      "Results with ngram_range: (1, 2) & alpha = 20\n",
      "Train: 0.9621342512908778\n",
      "Test: 0.655850540806293\n",
      "ridge done. Time spent so far...  0:01:35.249265 \n",
      "\n",
      "Results with ngram_range: (1, 2) & alpha = 1\n",
      "Train: 0.9936070813867716\n",
      "Test: 0.6411012782694199\n",
      "Results with ngram_range: (1, 2) & alpha = 2\n",
      "Train: 0.6788787804278338\n",
      "Test: 0.6607669616519174\n",
      "Results with ngram_range: (1, 2) & alpha = 3\n",
      "Train: 0.6833046471600689\n",
      "Test: 0.6607669616519174\n",
      "Results with ngram_range: (1, 2) & alpha = 4\n",
      "Train: 0.6643717728055077\n",
      "Test: 0.6607669616519174\n",
      "Results with ngram_range: (1, 2) & alpha = 5\n",
      "Train: 0.6678141135972461\n",
      "Test: 0.6607669616519174\n",
      "Results with ngram_range: (1, 2) & alpha = 6\n",
      "Train: 0.6633882468650111\n",
      "Test: 0.6607669616519174\n",
      "Results with ngram_range: (1, 2) & alpha = 7\n",
      "Train: 0.6636341283501352\n",
      "Test: 0.6607669616519174\n",
      "Tree count = 20      running...\n",
      "Tree count = 50      running...\n",
      "Tree count = 70      running...\n",
      "Tree count = 100      running...\n",
      "Results with ngram_range: (1, 2) & alpha = 50\n",
      "Train: 0.8630440127858372\n",
      "Test: 0.6312684365781711\n",
      "Results with ngram_range: (1, 2) & alpha = 100\n",
      "Train: 0.9188591099090239\n",
      "Test: 0.6243854473942969\n",
      "Results with ngram_range: (1, 2) & alpha = 200\n",
      "Train: 0.9675436439636096\n",
      "Test: 0.6066863323500492\n",
      "Results with ngram_range: (1, 2) & alpha = 300\n",
      "Train: 0.9936070813867716\n",
      "Test: 0.616519174041298\n",
      "\n",
      "______________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Results with ngram_range: (1, 3) & alpha = 0.01\n",
      "Train: 0.9936070813867716\n",
      "Test: 0.6538839724680433\n",
      "Results with ngram_range: (1, 3) & alpha = 0.1\n",
      "Train: 0.9936070813867716\n",
      "Test: 0.6538839724680433\n",
      "Results with ngram_range: (1, 3) & alpha = 1\n",
      "Train: 0.9936070813867716\n",
      "Test: 0.6529006882989183\n",
      "Results with ngram_range: (1, 3) & alpha = 5\n",
      "Train: 0.9933611999016474\n",
      "Test: 0.6509341199606686\n",
      "Results with ngram_range: (1, 3) & alpha = 10\n",
      "Train: 0.9931153184165232\n",
      "Test: 0.6538839724680433\n",
      "Results with ngram_range: (1, 3) & alpha = 20\n",
      "Train: 0.9850012294074256\n",
      "Test: 0.6548672566371682\n",
      "ridge done. Time spent so far...  0:05:29.667823 \n",
      "\n",
      "Results with ngram_range: (1, 3) & alpha = 1\n",
      "Train: 0.9936070813867716\n",
      "Test: 0.6420845624385447\n",
      "Results with ngram_range: (1, 3) & alpha = 2\n",
      "Train: 0.6707646914187362\n",
      "Test: 0.6617502458210422\n",
      "Results with ngram_range: (1, 3) & alpha = 3\n",
      "Train: 0.6729776247848537\n",
      "Test: 0.6617502458210422\n",
      "Results with ngram_range: (1, 3) & alpha = 4\n",
      "Train: 0.6626506024096386\n",
      "Test: 0.6607669616519174\n",
      "Results with ngram_range: (1, 3) & alpha = 5\n",
      "Train: 0.6638800098352594\n",
      "Test: 0.6607669616519174\n",
      "Results with ngram_range: (1, 3) & alpha = 6\n",
      "Train: 0.6626506024096386\n",
      "Test: 0.6607669616519174\n",
      "Results with ngram_range: (1, 3) & alpha = 7\n",
      "Train: 0.6626506024096386\n",
      "Test: 0.6607669616519174\n",
      "Tree count = 20      running...\n",
      "Tree count = 50      running...\n",
      "Tree count = 70      running...\n",
      "Tree count = 100      running...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5aa8fa4df59f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tree count ='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'     running...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mforest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mdictio\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'forest'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mdictio\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parameter'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    388\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    167\u001b[0m                                                         indices=indices)\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \"\"\"\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    899\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\BDA\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    387\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "begin_time = dt.datetime.now()\n",
    "\n",
    "df = science_df_clean\n",
    "results = []\n",
    "dictio = {}\n",
    "target = df.over_50\n",
    "\n",
    "for n in [(1,1), (1,2), (1,3), (2,2), (2,3), (3,3)]:\n",
    "    vectorizertf = TfidfVectorizer(stop_words = 'english', ngram_range = n)\n",
    "    vectorizerbow = CountVectorizer(stop_words = 'english', ngram_range = n)\n",
    "    vect = vectorizerbow.fit_transform(df.title)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(vect, target, test_size = 0.2, random_state = 2021)\n",
    "\n",
    "    for x in [0.01, 0.1, 1, 5, 10, 20]:\n",
    "        ridge = RidgeClassifier(alpha = x, max_iter=5000).fit(X_train, y_train)\n",
    "        print('Results with ngram_range: {} & alpha = {}\\nTrain: {}\\nTest: {}'.format(n, x, ridge.score(X_train, y_train), ridge.score(X_test, y_test)))\n",
    "        dictio['model'] = 'ridge'\n",
    "        dictio['parameter'] = x\n",
    "        dictio['train'] = ridge.score(X_train, y_train)\n",
    "        dictio['test'] = ridge.score(X_test, y_test)\n",
    "        dictio['ngrams'] = n\n",
    "        dictio_copy = dictio.copy()\n",
    "        results.append(dictio_copy)\n",
    "\n",
    "    print('ridge done. Time spent so far... ', dt.datetime.now() - begin_time, '\\n')\n",
    "\n",
    " \n",
    "    for x in range(1, 8):\n",
    "        knn = KNeighborsClassifier(n_neighbors = x, n_jobs=-1).fit(X_train, y_train)\n",
    "        print('Results with ngram_range: {} & alpha = {}\\nTrain: {}\\nTest: {}'.format(n, x, knn.score(X_train, y_train), knn.score(X_test, y_test)))\n",
    "        dictio['model'] = 'knn'\n",
    "        dictio['parameter'] = x\n",
    "        dictio['train'] = knn.score(X_train, y_train)\n",
    "        dictio['test'] = knn.score(X_test, y_test)\n",
    "        dictio['ngrams'] = n\n",
    "        dictio_copy = dictio.copy()\n",
    "        results.append(dictio_copy)\n",
    "\n",
    "    for x in [20, 50, 70, 100]:\n",
    "        print('Tree count =', x, '     running...')\n",
    "        for y in [50, 100, 200, 300]:\n",
    "            forest = RandomForestClassifier(n_estimators = x, max_depth = y).fit(X_train, y_train)\n",
    "            dictio['model'] = 'forest'\n",
    "            dictio['parameter'] = [x, y]\n",
    "            dictio['train'] = forest.score(X_train, y_train)\n",
    "            dictio['test'] = forest.score(X_test, y_test)\n",
    "            dictio['ngrams'] = n\n",
    "            dictio_copy = dictio.copy()\n",
    "            results.append(dictio_copy)\n",
    "\n",
    "    for y in [50, 100, 200, 300]:\n",
    "        dtree = DecisionTreeClassifier(max_depth = y).fit(X_train, y_train)\n",
    "        print('Results with ngram_range: {} & alpha = {}\\nTrain: {}\\nTest: {}'.format(n, y, dtree.score(X_train, y_train), dtree.score(X_test, y_test)))\n",
    "        dictio['model'] = 'dtree'\n",
    "        dictio['parameter'] = y\n",
    "        dictio['train'] = dtree.score(X_train, y_train)\n",
    "        dictio['test'] = dtree.score(X_test, y_test)\n",
    "        dictio['ngrams'] = n\n",
    "        dictio_copy = dictio.copy()\n",
    "        results.append(dictio_copy)\n",
    "    print('\\n______________________________________________________\\n\\n\\n')\n",
    "\n",
    "\n",
    "print('\\nALL DONE! - Total time spent training/testing (hh:mm:ss):', dt.datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in results:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_columns = ['model','parameter','train', 'test', 'ngrams']\n",
    "dict_data = results\n",
    "\n",
    "#CHANGE NAME BELOW---------------------------------------------------------------------\n",
    "csv_file = \"datasets\\\\model_results\\\\aggregate.csv\"\n",
    "#CHANGE NAME ABOVE---------------------------------------------------------------------\n",
    "with open(csv_file, 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "    writer.writeheader()\n",
    "    for data in dict_data:\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gaming_df\n",
    "target = df.score_class\n",
    "\n",
    "vectorizerbow = CountVectorizer(stop_words = 'english', ngram_range = n)\n",
    "vect = vectorizerbow.fit_transform(df.title)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vect, target, test_size = 0.2, random_state = 2021)\n",
    "\n",
    "ridge = RidgeClassifier(alpha = 1, max_iter=5000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe2 = science_df\n",
    "science_vect = vector.transform(dataframe2.title_cleaned)\n",
    "\n",
    "post_time = col_to_matrix(dataframe2, '24h_posttime')\n",
    "title_len = col_to_matrix(dataframe2, 'title length')\n",
    "has_body_text = col_to_matrix(dataframe2, 'has_body_text')\n",
    "\n",
    "science_vect = sparse.hstack((post_time, science_vect)) #Adding posttime column to matrix\n",
    "science_vect = sparse.hstack((title_len, science_vect)) #Adding title_length column to matrix\n",
    "science_vect = sparse.hstack((has_body_text, science_vect)) #Adding has_body_text column to matrix\n",
    "\n",
    "print(science_vect.shape)\n",
    "\n",
    "\n",
    "\n",
    "dataframe2 = gaming_df\n",
    "gaming_vect = vector.transform(dataframe2.title_cleaned)\n",
    "\n",
    "post_time = col_to_matrix(dataframe2, '24h_posttime')\n",
    "title_len = col_to_matrix(dataframe2, 'title length')\n",
    "has_body_text = col_to_matrix(dataframe2, 'has_body_text')\n",
    "\n",
    "gaming_vect = sparse.hstack((post_time, gaming_vect)) #Adding posttime column to matrix\n",
    "gaming_vect = sparse.hstack((title_len, gaming_vect)) #Adding title_length column to matrix\n",
    "gaming_vect = sparse.hstack((has_body_text, gaming_vect)) #Adding has_body_text column to matrix\n",
    "\n",
    "print(gaming_vect.shape)\n",
    "\n",
    "\n",
    "dataframe2 = sports_df\n",
    "sports_vect = vector.transform(dataframe2.title_cleaned)\n",
    "\n",
    "post_time = col_to_matrix(dataframe2, '24h_posttime')\n",
    "title_len = col_to_matrix(dataframe2, 'title length')\n",
    "has_body_text = col_to_matrix(dataframe2, 'has_body_text')\n",
    "\n",
    "sports_vect = sparse.hstack((post_time, sports_vect)) #Adding posttime column to matrix\n",
    "sports_vect = sparse.hstack((title_len, sports_vect)) #Adding title_length column to matrix\n",
    "sports_vect = sparse.hstack((has_body_text, sports_vect)) #Adding has_body_text column to matrix\n",
    "\n",
    "print(sports_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE IMPORTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in vectorizer.vocabulary_.items():\n",
    "    if value == 5069:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in vectorizerbow_out.vocabulary_.items():\n",
    "#     if value == coef.iloc[x].idxmin():\n",
    "#         word = key\n",
    "#     print(coef.iloc[x].loc[coef.iloc[x].idxmin()], word)\n",
    "\n",
    "# print()\n",
    "# for x in range(5):\n",
    "#     for key, value in vectorizerbow_out.vocabulary_.items():\n",
    "#         if value == coef.iloc[x].idxmax():\n",
    "#             word = key\n",
    "#     print(index[x], coef.iloc[x].loc[coef.iloc[x].idxmax()], word)\n",
    "# coef.iloc[4].loc[19272]\n",
    "coef = pd.DataFrame(ridge.coef_).T\n",
    "coef.rename(columns=({0:'one'}), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_vocab_sorted = dict(sorted(vectorizerbow_out.vocabulary_.items(), key=lambda item: item[1])) # Sorting dictionary of word:vector_index by vector_index\n",
    "\n",
    "vocab_dict = {}\n",
    "\n",
    "for x in range(len(vect_vocab_sorted)): #Creating a dictionary consisting of word:sum pairs\n",
    "    print(x)\n",
    "    vocab_dict[list(vect_vocab_sorted)[x]] = forest.feature_importances_[x+3]\n",
    "\n",
    "sorted_vocab = dict(sorted(vocab_dict.items(), key=lambda item: item[1], reverse=True)) #Sorting the word:sum pairs by sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(forest.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocab_items = sorted_vocab.items()\n",
    "cols = ['has_body_text', 'title_length', '24h_posttime']\n",
    "for x in range(3):\n",
    "    print(cols[x], ':', forest.feature_importances_[x])\n",
    "list(sorted_vocab_items)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.feature_importances_[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-064e6ddd0b9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;34m'body'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'https://www.nba.com/schedule'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Arent your sleepy my guy?'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nan'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'https://www.youtube.com/watch?v=hruqhM2Axnc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             'timestamp': ['2021-05-26 21:03:19', '2021-05-26 02:03:19', '2021-05-26 18:03:19', '2021-05-26 20:03:19']}\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mcreate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mpreprocess_text_col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "test_data = {'title':['Looking forward to the amazing match between the suns and lakers tonight! Lebron will without a doubt be an issue for suns, but never dismiss the sheer force that is Deandre Ayton!!! GO SUNS', \n",
    "                     'Im going to sleep soon. see you tommorow yall',\n",
    "                      'So ill just write a reaaaaally long text for this title and see how it performs. Maybe long titles just give better results? who am i to say. This wont include any body text either..', \n",
    "                      'Dodging a cash-in-transit robbery. The man has balls of steel'], \n",
    "            'body':['https://www.nba.com/schedule', 'Arent your sleepy my guy?', 'nan', 'https://www.youtube.com/watch?v=hruqhM2Axnc'],\n",
    "            'timestamp': ['2021-05-26 21:03:19', '2021-05-26 02:03:19', '2021-05-26 18:03:19', '2021-05-26 20:03:19']}\n",
    "test_df = pd.DataFrame(test_data, columns=['title', 'body', 'timestamp'])\n",
    "create_features(test_df)\n",
    "preprocess_text_col(test_df, 'title')\n",
    "print(test_df.head())\n",
    "test_vect = vector.transform(test_df.title_cleaned)\n",
    "\n",
    "post_time = col_to_matrix(test_df, '24h_posttime')\n",
    "title_len = col_to_matrix(test_df, 'title length')\n",
    "has_body_text = col_to_matrix(test_df, 'has_body_text')\n",
    "\n",
    "test_vect = sparse.hstack((post_time, test_vect)) #Adding posttime column to matrix\n",
    "test_vect = sparse.hstack((title_len, test_vect)) #Adding title_length column to matrix\n",
    "test_vect = sparse.hstack((has_body_text, test_vect)) #Adding has_body_text column to matrix\n",
    "\n",
    "ridge.predict(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in results.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"datasets\\\\\\model_results\\\\unigram_master.csv\", \"w\")\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for k, v in results.items():\n",
    "    writer.writerow([k, v])\n",
    "\n",
    "a_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
